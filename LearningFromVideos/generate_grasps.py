"""
This file contains the functions necessary to generate grasps by iterating through 
each demo in a given dataset, generating and saving a camera pose/image of the object
in each demo, and saving a set of grasps generated by the camera image. 
"""

import os
import json
import h5py
import numpy as np
import robosuite
import robosuite.robosuite.utils.camera_utils as CamUtils
import subprocess

def get_dataset(dataset_path):
    """
    Given hdf5 dataset path, retrieve the file, list of demos, and environment metadata.
    Demos are sourced from robomimic v0.1 dataset.

    Args:
        dataset_path (string): path to hdf5 dataset

    Returns:
        3-tuple:
            - (file) hdf5 file pointed to by dataset_path
            - (list) list of demo names as strings
            - (dict) dictionary containing environment metadata
    """

    assert os.path.exists(dataset_path)
    f = h5py.File(dataset_path, "r")

    demos = list(f["data"].keys())

    inds = np.argsort([int(elem[5:]) for elem in demos])
    demos = [demos[i] for i in inds]

    env_meta = json.loads(f["data"].attrs["env_args"])

    return f, demos, env_meta

def make_env(env_meta):
    """
    Create a robosuite environment with an 'OSC_POSE' controller based on the environment metadata of the original demonstration.

    Args:
        env_meta (dict): dictionary containing environment metadata

    Returns:
        3-tuple:
            robosuite.env: initialized robosuite environment
            np.array: intrinsic camera matrix
            CamUtils.CameraMover: camera mover object to set position of camera in sim
    """

    env = robosuite.robosuite.make(
        env_meta['env_name'],
        robots=env_meta['env_kwargs']['robots'],
        gripper_types="default", 
        controller_configs=env_meta['env_kwargs']['controller_configs'],
        env_configuration="single-arm-opposed",
        has_renderer=False,     
        has_offscreen_renderer=True,         
        control_freq=20,
        use_object_obs=True,
        use_camera_obs=True,
        camera_names="birdview",   
        camera_depths=True,
        camera_heights=1024,
        camera_widths=1024,
        horizon=3000                
    )

    K = CamUtils.get_camera_intrinsic_matrix(env.sim, "birdview", 1024, 1024)
    cam_mover = CamUtils.CameraMover(env, camera="birdview")

    return env, K, cam_mover


def generate_rgbd(f, env, ep, K, cam_mover):
    """
    Generates an RGBD image of an object to feed into Contact Grasp-Net

    Args:
        f (file): hdf5 file containing demo
        env (robosuite.env): environment in which the task is executed
        ep (string): name of demo for which grasps are generated
        K (np.array): intrinsic camera matrix
        cam_mover (CamUtils.CamMover): camera mover object to set position of camera in sim
    
    Returns:
        2-tuple:
            - np.array: camera position
            - np.array: camera quaternion
    """

    print('Generating rgbd point cloud for {}'.format(ep))
    
    # set environment to initial state
    initial_state = f["data/{}/states".format(ep)][0]
    actions = f["data/{}/actions".format(ep)]
    env.sim.set_state_from_flattened(initial_state)
    env.sim.forward()

    # move robot arm out of the way
    neutral = np.array([0, 0, 1, 0, 0, 0, 0])
    for i in range(10):
        obs, _, _, _ = env.step(neutral)

    # set camera pose to be 0.25m directly above nut, pointing down
    cam_pos = obs['SquareNut_pos'] + [0, 0, 0.25]
    cam_quat = obs['SquareNut_quat']
    cam_mover.set_camera_pose(pos=cam_pos, quat=cam_quat)

    # move camera so that it is looking down on the nut
    cam_mover.rotate_camera(point=None, axis=(0, 0, 1), angle=45)
    cam_mover.move_camera((0, 1, 0), -0.15)
    cam_mover.rotate_camera(point=None, axis=(1, 0, 0), angle=30)
    
    # get camera pose
    cam_pos, cam_quat = cam_mover.get_camera_pose()

    # get depth map and rgb image
    depth = CamUtils.get_real_depth_map(env.sim, np.squeeze(obs['birdview_depth']))
    rgb = obs['birdview_image']

    # flip depth map and rgb image to align grasp generation and robosuite environment
    depth_flipped = depth[::-1, :]
    rgb_flipped = rgb[::-1, :, :]

    # save in Contact-GraspNet's desired format
    env_point_cloud = {'depth': depth_flipped, 'K': K, 'rgb': rgb_flipped}
    np.save('meshes/square_flipped_{}.npy'.format(ep), env_point_cloud)

    print("Successfully generated rgdb point cloud for {}".format(ep))

    return np.concatenate((cam_pos, cam_quat))

def generate_grasps(demo):
    """
    Run Contact Grasp-Net's terminal command to generate grasps

    Args:
        demo (string): name of demo
    """
    print('Generating grasps for {}'.format(demo))
    rgbd_path = '../meshes/square_flipped_{}.npy'.format(demo)
    working_directory = 'contact_graspnet/'
    subprocess.run(["conda", "run", "-n", "contact_graspnet_env", "python", "contact_graspnet/inference.py", "--np_path={}".format(rgbd_path)], cwd=working_directory)
    print("Grasp generation successfully finished for {}".format(demo))

def main():
    dataset_path = 'datasets/square/ph/demo_v141.hdf5'
    f, demos, env_meta = get_dataset(dataset_path)
    env, K, cam_mover = make_env(env_meta)
    
    cam_poses = np.empty((len(demos), 7))

    for i in range(len(demos)):
        cam_pose = generate_rgbd(f, env, demos[i], K, cam_mover)
        print("Cam pose {}:".format(i), cam_pose)
        generate_grasps(demos[i])
        cam_poses[i] = cam_pose

    env.close()

    np.save('grasps/square/cam_poses_2.npy', cam_poses)
    
    
if __name__ == "__main__":
    main()