"""
This file loads in the environment from each demonstration in a demo file, a set of corresponding grasps, 
and a camera pose, and executes each grasp in the environemnt, tracking the success of each grasp.
"""

import os
import json
import h5py
import numpy as np
import pdb
import imageio
import matplotlib.pyplot as plt
from time import sleep
import sys

import robosuite as suite
from robosuite.robosuite.utils.input_utils import *
import robosuite.robosuite.utils.camera_utils as CamUtils
from robosuite.robosuite.utils.transform_utils import euler2mat, mat2quat, quat2axisangle, quat2mat

camera_id = 0

RENDER = True

def flip(matrix):
    """
    Helper function to flip a matrix.

    Args:
        matrix (np.array): any 2D+ matrix

    Returns:
        np.array: flipped matrix
    """
    reversed_rows = np.flipud(matrix)
    return reversed_rows

def get_dataset(dataset_path):
    """
    Given hdf5 dataset path, retrieve the file, list of demos, and environment metadata.
    Demos are sourced from robomimic v0.1 dataset.

    Args:
        dataset_path (string): path to hdf5 dataset

    Returns:
        3-tuple:
            - (file) hdf5 file pointed to by dataset_path
            - (list) list of demo names as strings
            - (dict) dictionary containing environment metadata
    """
    assert os.path.exists(dataset_path)
    f = h5py.File(dataset_path, "r")

    demos = list(f["data"].keys())

    inds = np.argsort([int(elem[5:]) for elem in demos])
    demos = [demos[i] for i in inds]

    env_meta = json.loads(f["data"].attrs["env_args"])

    return f, demos, env_meta

def make_env(env_meta):
    """
    Create a robosuite environment with an 'OSC_POSE' controller based on the environment metadata of the original demonstration.

    Args:
        env_meta (dict): dictionary containing environment metadata

    Returns:
        robosuite.env: initialized robosuite environment
    """

    # Set controller configs
    env_meta['env_kwargs']['controller_configs']['control_delta'] = False
    env_meta['env_kwargs']['controller_configs']['kp'] = [75, 50, 50, 150, 150, 150, 150]
    env_meta['env_kwargs']['controller_configs']['damping_ratio'] = 3

    render_env = suite.robosuite.make(
        env_meta['env_name'],
        robots=env_meta['env_kwargs']['robots'],
        gripper_types="default", 
        controller_configs=env_meta['env_kwargs']['controller_configs'],
        env_configuration="single-arm-opposed",
        has_renderer=RENDER,     
        has_offscreen_renderer=not RENDER,  
        ignore_done=False,        
        control_freq=20,
        horizon=300,
        use_object_obs=True,
        use_camera_obs=False
    
        # set camera observations if offscreed renderer
        #use_camera_obs=True,
        #camera_names='agentview',
        #camera_heights=1024,
        #camera_widths=1024,
    )
    return render_env

def get_cam_poses():
    """
    Load camera poses from .npy file.

    Returns:
        np.array: array of camera poses generated by generate_grasps.py
    """
    
    cam_poses = np.load('grasps/square/cam_poses_0.npy')
    return cam_poses

def get_grasps(demo):
    """
    Load grasps generated by contact graspnet from .npz file.

    Args:
        demo (string): name of demo

    Returns:
        np.array: numpy array of grasps generated for each demo
    """
    grasps = np.load('grasps/square/predictions_square_flipped_{}.npz'.format(demo), allow_pickle=True)
    return grasps['pred_grasps_cam'].item()[-1]

def calc_grasp_pose(cam_pose, T_BtoD):
    """
    Given a cam pose and a transformation matrix from grasp to camera, calculates the eef pose of the grasp
    with respect to the world frame.

    Args:
        cam_pose (np.array): camera pose in [pos, quat] format
        T_BtoD (np.array): transformation matrix from grasp to camera

    Returns:
        np.array: action given to robot to move EEF to grasp pose in [pos, axis-angle] format.
    """
    T_DtoC = np.eye(4)
    T_DtoC[1][1] = -1
    T_DtoC[2][2] = -1

    R_CtoN = quat2mat(cam_pose[3:]) 
    T_CtoN = np.eye(4)
    T_CtoN[:3, :3] = R_CtoN
    T_CtoN[:3, 3] = cam_pose[:3]

    """
    T_CtoN: Transformation from Camera to World axis
    T_DtoC: Transformation from OpenCV to OpenGL coordinates
    T_BtoD: Transformation given by Contact Graspnet from grasp to camera
    """

    T_NtoB = T_CtoN @ T_DtoC @ T_BtoD
    R_NtoB = T_NtoB[:3, :3]
    r_NtoB = T_NtoB[:3, 3]
    rot = quat2axisangle(mat2quat(R_NtoB))

    return np.concatenate((r_NtoB, rot), axis=0)

def move_to_grasp(f, ep, env, trajectory, writer=None):
    """
    Moves EEF to initial grasp state.

    Args:
        f (file): hdf5 file containing demo
        ep (string): name of demo for which grasps are generated
        env (robosuite.env): environment in which the tast is executed
        trajectory (np.array): action given to environment to command EEF to desired pose

    Returns:
        2-tuple:
            - (np.array): EEF pose after action
            - (np.array): Nut pose after action
    """

    print("Moving to grasp")

    # reset environment and set it to the demo's initial state
    obs = env.reset()
    initial_state = f["data/{}/states".format(ep)][0]
    env.sim.set_state_from_flattened(initial_state)
    env.sim.forward()


    eef_pose = np.concatenate((obs['robot0_eef_pos'], obs['robot0_eef_quat']), axis=0) 
    print("Initial EEF Pose:", eef_pose)
    print("Expected Pre-grasp Pose:", trajectory)

    # define action, make a step to render
    action = np.zeros(7)
    action[:6] = trajectory + np.array([0.02, -0.02, -0.1, 0, 0, 0])
    action[6] = -1
    obs, _, _, _ = env.step(action)

    if RENDER:
        env.viewer.set_camera(camera_id=camera_id)
        env.viewer.render()
    else:
        writer.append_data(flip(obs["agentview_image"]))
    
    # Move to pose that is 0.1m above the grasp pose to avoid horizontal collision
    for i in range(25):
        obs, _, _, _ = env.step(action + np.array([0, 0, 0.1, 0, 0, 0, 0]))
        
        if RENDER:
            env.viewer.set_camera(camera_id=camera_id)
            env.viewer.render()
        else:
            writer.append_data(flip(obs["agentview_image"]))
    
    # Move to grasp pose
    for i in range(100):
        obs, _, _, _ = env.step(action) 
        
        if RENDER:
            env.viewer.set_camera(camera_id=camera_id)
            env.viewer.render()
        else:
            writer.append_data(flip(obs["agentview_image"]))


    eef_pose = np.concatenate((obs['robot0_eef_pos'], obs['robot0_eef_quat']), axis=0)
    nut_pose = np.concatenate((obs['SquareNut_pos'], obs['SquareNut_quat']), axis=0)
    print("Nut pose:", nut_pose)
    print("Pre-grasp EEF Pose:", eef_pose)    

    # Close gripper
    action[6] = 1
    for i in range(25):
        obs, _, _, _ = env.step(action)

        if RENDER:
            env.viewer.set_camera(camera_id=camera_id)
            env.viewer.render()
        else:
            writer.append_data(flip(obs["agentview_image"]))


    print("Finished moving to grasp")

    return eef_pose, nut_pose

def get_object_trajectory(demo):
    """
    Loads demo trajectory from set generated by paired_dataset.py.

    Args:
        demo (string): name of demonstration

    Returns:
        2-tuple:
            - np.array: list of object positions at each timestep
            - np.array: list of object quaternioins at each timestep
    """

    trajectories = np.load('rm_paired_dataset/demo_trajectories.pkl', allow_pickle=True)

    pos_list = trajectories['object_locations'][int(demo[5:])]
    quat_list = trajectories['object_quaternions'][int(demo[5:])]

    return pos_list, quat_list

def create_dummy_pose_lists(initial_nut_pose):
    """
    Creates a list of dummy poses that can be used instead of a demo trajectory.

    Args:
        initial_nut_pose (np.array): Initial nut pose in the format [pos, quat]
    
    Returns:
        2-tuple:
            - np.array: list of object positions at each timestep
            - np.array: list of object quaternioins at each timestep
    """

    end_nut_pose = np.concatenate(([0.24, 0.1, 0.85], initial_nut_pose[3:]))
    mid_nut_pose = np.concatenate(([0.24, 0.1, 1], initial_nut_pose[3:]))

    pos_list = []
    quat_list = []

    for i in range(100):
        pos_list.append(initial_nut_pose[:3] + (mid_nut_pose[:3] - initial_nut_pose[:3]) * i / 99)
        quat_list.append(mid_nut_pose[3:])

    for k in range(25):
        pos_list.append(mid_nut_pose[:3])
        quat_list.append(mid_nut_pose[3:])

    for j in range(100):
        pos_list.append(mid_nut_pose[:3] + (end_nut_pose[:3] - mid_nut_pose[:3]) * j / 99)
        quat_list.append(end_nut_pose[3:])

    for k in range(25):
        pos_list.append(end_nut_pose[:3])
        quat_list.append(end_nut_pose[3:])

    return pos_list, quat_list

def compute_EtoO(eef_pose, nut_pose):
    """
    Helper function that computes the translation matrix fron EEF pose to object pose.

    Args:
        eef_pose (np.array): EEF pose in [pos, quat] format
        nut_pose (np.array): nut pose in [pose, quat] format

    Returns:
        np.array: Transformation matrix from EEF pose to nut pose
    """
    # T_EtoN: transformation matrix from EEF to world frame
    # T_OtoN: transformation matrix from object to world frame
    # T_EtoO: transformation matrix from EEF to object

    T_EtoN = np.eye(4)
    T_EtoN[:3, :3] = quat2mat(eef_pose[3:])
    T_EtoN[:3, 3] = eef_pose[:3]

    T_OtoN = np.eye(4)
    T_OtoN[:3, :3] = quat2mat(nut_pose[3:])
    T_OtoN[:3, 3] = nut_pose[:3]

    T_EtoO = np.linalg.inv(T_OtoN) @ T_EtoN

    return T_EtoO

def follow_object_trajectory(pos_list, quat_list, initial_eef_pose, initial_nut_pose, env, stats, grasp, writer=None):
    """
    Moves EEF post-grasp to follow a given object trajectory.

    Args:
        pos_list (np.array): list of object positions at each timestep
        quat_list (np.array): list of object quaternioins at each timestep
        initial_eef_pose (np.array): pose of EEF post-grasp
        initial_nut_pose (np.array): pose of nut post-grasp
        env (robosuite.env): robosuite environment when actions are made
        stats (list): list of failed grasps, unsuccessful trajectories, and successful trajectories
        grasp (int): grasp index that is being tested

    Returns:
        list: updated list of stats
    """
    print("Following object trajectory")

    actual_x = []
    demo_x = []
    actual_y = []
    demo_y = []
    actual_z = []
    demo_z = []
    t = []

    # Compute current transformation matrix from EEF to nut
    T_EtoO = compute_EtoO(initial_eef_pose, initial_nut_pose)
    # Compute next transformation matrix and action from EEF to world
    T_OtoN = np.eye(4)
    T_OtoN[:3, :3] = quat2mat(quat_list[0])
    T_OtoN[:3, 3] = pos_list[0]
    T_EtoN = T_OtoN @ T_EtoO
    # Transformation must be rotated -pi/2 about the x-axis to comply with offset between EEF and gripper
    T_EtoN[:3, :3] = T_EtoN[:3, :3] @ euler2mat(np.array([0, 0, -np.pi/2]))
    action = np.concatenate((T_EtoN[:3, 3], quat2axisangle(mat2quat(T_EtoN[:3, :3])), [1]), axis=0)

    obs, _, _, _ = env.step(action)

    if RENDER:
        env.viewer.set_camera(camera_id=camera_id)
        env.viewer.render()
    else:
        writer.append_data(flip(obs["agentview_image"]))

    # deceleration: how many steps are taken to reach each trajectory timestep
    deceleration = 3

    failed_grasp = False

    # iterate through list of positions and execute robot action to move to next position
    for i in range(len(pos_list) * deceleration):
        # retrieve next expected transformation from object to world frame.
        T_OtoN[:3, :3] = quat2mat(quat_list[i // deceleration])
        T_OtoN[:3, 3] = pos_list[i // deceleration]

        # retrieve the current EEF and nut poses
        next_eef_pose = np.concatenate((obs['robot0_eef_pos'], obs['robot0_eef_quat']), axis=0)
        next_nut_pose = np.concatenate((obs['SquareNut_pos'], obs['SquareNut_quat']), axis=0)
        
        # end rollout if graps failed
        if obs['robot0_gripper_qpos'][0] < 0.001 and i < 100 * deceleration:
            stats[0] += 1
            print("Failed grasp on grasp", grasp)
            failed_grasp = True
            break
        
        # Compute next transformation matrix and action from EEF to world
        T_EtoO = compute_EtoO(next_eef_pose, next_nut_pose)
        T_EtoN = T_OtoN @ T_EtoO
        T_EtoN[:3, :3] = T_EtoN[:3, :3] @ euler2mat(np.array([0, 0, -np.pi/2]))
        action = np.concatenate((T_EtoN[:3, 3], quat2axisangle(mat2quat(T_EtoN[:3, :3])), [1]), axis=0)

        obs, _, _, _ = env.step(action)

        if RENDER:
            env.viewer.set_camera(camera_id=camera_id)
            env.viewer.render()
        else:
            writer.append_data(flip(obs["agentview_image"]))

        actual_x.append(obs['SquareNut_pos'][0])
        demo_x.append(pos_list[i // 3][0])
        actual_y.append(obs['SquareNut_pos'][1])
        demo_y.append(pos_list[i // 3][1])
        actual_z.append(obs['SquareNut_pos'][2])
        demo_z.append(pos_list[i // 3][2])
        t.append(i)

    # finish trajectory with 10 additional steps in final timestep
    for i in range(10):
        obs, _, _, _ = env.step(action)

        if RENDER:
            env.viewer.set_camera(camera_id=camera_id)
            env.viewer.render()
        else:
            writer.append_data(flip(obs["agentview_image"]))

        actual_x.append(obs['SquareNut_pos'][0])
        demo_x.append(pos_list[i // 3][0])
        actual_y.append(obs['SquareNut_pos'][1])
        demo_y.append(pos_list[i // 3][1])
        actual_z.append(obs['SquareNut_pos'][2])
        demo_z.append(pos_list[i // 3][2])
        t.append(i + len(pos_list) * deceleration)

    # calculate stats based on distance between nut pose and expected nut pose
    if not failed_grasp:
        if 0.2 < obs['SquareNut_pos'][0] < 0.25 and 0.09 < obs['SquareNut_pos'][1] < 0.14 and obs['SquareNut_pos'][2] < 0.86:
            stats[2] += 1
            print("Successful trajectory on grasp", grasp)
        else:
            stats[1] += 1
            print("Unsuccessful trajectory on grasp", grasp)

    # plot demo to actual results for data analysis
    plt.plot(t, actual_x, color='red')
    plt.plot(t, demo_x, color='blue')
    plt.show()
    plt.plot(t, actual_y, color='red')
    plt.plot(t, demo_y, color='blue')
    plt.show()
    plt.plot(t, actual_z, color='red')
    plt.plot(t, demo_z, color='blue')
    plt.show()

    print("Finished following object trajectory")

    return stats

def test_grasps_for_ep(cam_pose, grasps, f, demo, render_env, stats):
    """
    Test all generated grasps for given episode by executing each one in environment.

    Args:
        cam_pose (np.array): camera pose for this episode in [pos, quat] format
        grasps (np.array): array of all grasps that were generated for this episode
        f (file): hdf5 file containing demo
        demo (string): name of demo for which grasps are generated
        render_env (robosuite.env): environment in which the tast is executed
        stats (list): list of grasp success stats so far

    Returns:
        list: list of grasp success stats after episode
    """

    pos_list, quat_list = get_object_trajectory(demo)
    
    if not RENDER:
        writer = imageio.get_writer('grasp_test_{}.mp4'.format(demo), fps=20)

    # iterate through grasps and test each one by moving to grasp pose and attempting to follow demo trajectory
    for i in range(len(grasps)):
        print('Generating grasp', i)
        print('Cam Pose:', cam_pose)
        trajectory = calc_grasp_pose(cam_pose, grasps[i])
        eef_pose, nut_pose = move_to_grasp(f, demo, render_env, trajectory)
        stats = follow_object_trajectory(pos_list, quat_list, eef_pose, nut_pose, render_env, stats, i)
    
    if not RENDER:
        writer.close()

    render_env.close()

    print("Stats so far:", stats)

    return stats    

def main():
    dataset_path = 'datasets/square/ph/low_dim.hdf5'
    f, demos, env_meta = get_dataset(dataset_path)
    
    cam_poses = get_cam_poses()

    stats = np.zeros(3)

    for ep in range(2, len(demos)):
        render_env = make_env(env_meta)
        grasps = get_grasps(demos[ep])
        cam_pose = cam_poses[ep] 
        if (len(grasps) < 50):
            stats = test_grasps_for_ep(cam_pose, grasps, f, demos[ep], render_env, stats)

    print("Failed grasps:", stats[0])
    print("Unsuccessful trajectories:", stats[1])
    print("Successful trajectories:", stats[2])

    np.save('grasp_stats', stats)

if __name__ == "__main__":
    main()